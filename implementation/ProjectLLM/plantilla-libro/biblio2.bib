@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{IBM-LLM,
    author = "IBM",
    title = "¿Qué son los grandes modelos de lenguaje (LLM)?",
    url = "https://www.ibm.com/es-es/topics/large-language-models",
    note = "Accedido el 20 de abril de 2024"
}
@online{AWS-LLM,
    author = "AWS",
    title = "¿Qué son los grandes modelos de lenguaje (LLM)?",
    url = "https://aws.amazon.com/es/what-is/large-language-model/",
    note = "Accedido el 24 de abril de 2024"
}
@online{Shaip-LLM,
    author = "Shaip",
    title = 
        "Modelos de lenguaje grande (LLM): guía completa en 2023",
    url = "https://es.shaip.com/blog/a-guide-large-language-model-llm/",
    note = "Accedido el 24 de abril de 2024"
}
@online{IBM-RNN,
    author = "IBM",
    title = "¿Qué son las redes neuronales recurrentes?"
        ,
    url = "https://www.ibm.com/mx-es/topics/recurrent-neural-networks",
    note = "Accedido el 20 de abril de 2024"
}
@article{atention-blog,
    author ="Pierrick RUGERY" ,
    title ="Attention is all you need" ,
    journal ="Becoming Human: Artificial Intelligence Magazine" ,
    year = "2020"
}
@online{video-youtube,
    author       = {Dot-csv},
    title        = {¿Qué es un TRANSFORMER? La Red Neuronal que lo cambió TODO!
},
    year         = {2022},
    url          = {https://www.youtube.com/watch?v=aL-EmKuB078&t=840s},
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@online{gpt,
    author       = {TechTarget},
    title        = {Exploring GPT-3 architecture},
    year         = {2024},
    url          = {https://www.techtarget.com/searchenterpriseai/feature/Exploring-GPT-3-architecture},
}

@online{gpt-architecture,
    author       = {Ajitesh Kumar},
    title        = {BERT vs GPT Models: Differences, Examples},
    year         = {2024},
    url          = {https://vitalflux.com/bert-vs-gpt-differences-real-life-examples/},
}
@online{UMA,
    author       = "UMA",
    title        = {Málaga será un laboratorio referente en la aplicación de la inteligencia artificial a la investigación oncológica},
    year         = {2022},
    url          = {https://www.uma.es/sala-de-prensa/noticias/malaga-sera-un-laboratorio-referente-en-la-aplicacion-de-la-inteligencia-artificial-la-investigacion-oncologica/?set_language=en},
}
